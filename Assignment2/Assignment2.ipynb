{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvig’s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "MoQeEsZvHvvi",
    "ExecuteTime": {
     "start_time": "2024-03-22T23:50:31.888612Z",
     "end_time": "2024-03-22T23:50:35.391431Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "def load_ngram_model(ngram_file):\n",
    "    ngram_model = {}\n",
    "    with open(ngram_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            context = parts[1]\n",
    "            word = parts[2]\n",
    "            count = int(parts[0])\n",
    "            if context not in ngram_model:\n",
    "                ngram_model[context] = {}\n",
    "            ngram_model[context][word] = count\n",
    "    for i in ngram_model.keys():\n",
    "        summa = 0\n",
    "        for j in ngram_model[i].keys():\n",
    "            summa += ngram_model[i][j]\n",
    "        ngram_model[i][\"_values_sum\"] = summa\n",
    "\n",
    "    return ngram_model\n",
    "\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "NGRAM_MODEL = load_ngram_model('bigrams.txt')\n",
    "N = sum(WORDS.values())\n",
    "\n",
    "\n",
    "def P_word(word):\n",
    "    \"Frequency probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def P_context(word, context, default=0.001):\n",
    "    \"Probability of word given the `context` of 1 word since I use bigram model\"\n",
    "    if word in NGRAM_MODEL[context].keys():\n",
    "        return NGRAM_MODEL[context][word] / NGRAM_MODEL[context][\"_values_sum\"]\n",
    "    else:\n",
    "        return 1 / NGRAM_MODEL[context][\"_values_sum\"]\n",
    "\n",
    "\n",
    "def correction(word, context=None):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    if context not in NGRAM_MODEL.keys():\n",
    "        context = None\n",
    "    return max(candidates(word), key=lambda x: P_word(x) * P_context(x, context) if context else P_word(x))\n",
    "\n",
    "\n",
    "def candidates(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "'sport'"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of correction with the use of context info\n",
    "correction('spors', \"big\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:44.765093Z",
     "end_time": "2024-03-22T22:44:44.780128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "'spurs'"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correction without context\n",
    "correction('spors')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:44.780128Z",
     "end_time": "2024-03-22T22:44:44.905027Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "I decided to use bigram dataset because of its simplicity and versatility. The words without context(without preceding word) treated the same as in original Norvig’s solution. But if there is preceding word, we calculate the best spelling correction as a product of frequency probability of the word itself and probability of the word given the context. If the context do not have this particular word, it means that it is rare. Therefore, context probability for it would be 1/(total number of words for that context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Biulding test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "# Misspellings dictionary\n",
    "errors = {}\n",
    "with open(\"missp.dat.txt\", \"r\") as file:\n",
    "    line = file.readline()\n",
    "    curr_word = None\n",
    "    while line:\n",
    "        line = line.lower().strip()\n",
    "        if line[0] == \"$\":\n",
    "            errors[line[1:]] = []\n",
    "            curr_word = line[1:]\n",
    "        else:\n",
    "            errors[curr_word].append(line)\n",
    "        line = file.readline()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:44.796181Z",
     "end_time": "2024-03-22T22:44:45.026305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "['afful',\n 'aglift',\n 'aleal',\n 'alfal',\n 'alfall',\n 'alfaul',\n 'alfol',\n 'alfool',\n 'alfory',\n 'alful',\n 'alfule',\n 'alfull',\n 'allfall',\n 'allful',\n 'allfull',\n 'allfulw',\n 'althouge',\n 'although',\n 'alwafull',\n 'alwful',\n 'alwy',\n 'arfall',\n 'arful',\n 'arfull',\n 'aufal',\n 'aufall',\n 'aufel',\n 'aufful',\n 'aufle',\n 'auful',\n 'aufull',\n 'aughfull',\n 'aweful',\n 'awefull',\n 'awfal',\n 'awfall',\n 'awfull',\n 'awlful',\n 'awufl',\n 'eafall',\n 'ofull',\n 'olfal',\n 'orfal',\n 'orfall',\n 'orfortil',\n 'orful',\n 'orfull',\n 'orsowl',\n 'orthell',\n 'ouful']"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of misspellings dictionary entry\n",
    "errors[\"awful\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:44.875977Z",
     "end_time": "2024-03-22T22:44:45.031283Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(333)\n",
    "\n",
    "test_set = []\n",
    "with open(\"big.txt\", \"r\") as file:\n",
    "    line = file.readline()\n",
    "    while line:\n",
    "        line = line.lower().strip()\n",
    "        if len(line) > 1:\n",
    "            splitted = line.split()\n",
    "            if len(splitted) > 1:\n",
    "                for i in range(1, len(splitted)):\n",
    "                    if splitted[i].strip() in errors.keys():\n",
    "                        context = splitted[i - 1].strip()\n",
    "                        if not context.isalpha():  # Exclude numerical data\n",
    "                            continue\n",
    "                        correct = splitted[i].strip()\n",
    "                        wrong = random.choice(errors[correct]).strip()\n",
    "                        test_set.append((context, wrong, correct))\n",
    "        line = file.readline()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:44.896297Z",
     "end_time": "2024-03-22T22:44:47.568391Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "609224"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:47.566054Z",
     "end_time": "2024-03-22T22:44:47.574439Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "# Let's decrease the test set to speed up the testing\n",
    "test_set = random.sample(test_set, 10000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:47.574439Z",
     "end_time": "2024-03-22T22:44:47.646787Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "[('and', 'ro', 'to'),\n ('as', 'as', 'a'),\n ('altercation', 'occureed', 'occurred'),\n ('without', 'nearse', 'any'),\n ('evident', 'wis', 'wish'),\n ('in', 'compy', 'company'),\n ('before', 'hte', 'the'),\n ('the', 'men', 'man'),\n ('on', 't', 'the'),\n ('is', 'beat', 'best')]"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of test data\n",
    "test_set[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:47.641118Z",
     "end_time": "2024-03-22T22:44:47.658388Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3173 Time:  0:00:48.262752\n"
     ]
    }
   ],
   "source": [
    "# Testing my solution\n",
    "import datetime\n",
    "\n",
    "begin = datetime.datetime.now()\n",
    "corrects = 0\n",
    "for i in test_set:\n",
    "    result = correction(i[1], i[0])\n",
    "    if result == i[2]:\n",
    "        corrects += 1\n",
    "end = datetime.datetime.now()\n",
    "print(\"Accuracy: \", corrects / len(test_set), \"Time: \", end - begin)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:44:47.658388Z",
     "end_time": "2024-03-22T22:45:35.933970Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "# Now let's compare with Norvig's original solution\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def norwig_correction(word):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:45:35.936092Z",
     "end_time": "2024-03-22T22:45:35.941692Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.2874 Time:  0:00:36.779969\n"
     ]
    }
   ],
   "source": [
    "begin = datetime.datetime.now()\n",
    "corrects = 0\n",
    "for i in test_set:\n",
    "    result = norwig_correction(i[1])\n",
    "    if result == i[2]:\n",
    "        corrects += 1\n",
    "end = datetime.datetime.now()\n",
    "print(\"Accuracy: \", corrects / len(test_set), \"Time: \", end - begin)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T22:45:35.944751Z",
     "end_time": "2024-03-22T22:46:12.751725Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### As a result, I was able to achieve greater accuracy in spelling correction by adding the use of the bigram model to the original Norvig's solution. However, the execution time of my solution is ~30% slower, which is expected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "def correct_text_lines(text):\n",
    "    text = text.lower()\n",
    "    result = []\n",
    "    text = text.split(\"\\n\")\n",
    "    for line in text:\n",
    "        if len(line) == 0:\n",
    "            result.append(line)\n",
    "            continue\n",
    "        splitted = line.split()\n",
    "        if len(splitted) == 1:\n",
    "            result.append(correction(splitted[0], None))\n",
    "            continue\n",
    "\n",
    "        temp = [correction(splitted[0], None)]\n",
    "        for i in range(1, len(splitted)):\n",
    "            temp.append(correction(splitted[i], splitted[i-1]))\n",
    "        result.append(\" \".join(temp))\n",
    "    return \"\\n\".join(result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T23:47:01.525520Z",
     "end_time": "2024-03-22T23:47:01.563187Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of the adventures of sherlock holmes\n",
      "by sir arthur conan doyle\n",
      "15 in out series by sir arthur conan doyle\n",
      "\n",
      "copyright laws are changing all over the world be sure to check the\n",
      "copyright laws for your country before downloading or redistributing\n",
      "this or any other project gutenberg ebook\n",
      "\n",
      "this header should be the first thing seen when viewing this project\n",
      "gutenberg file please do not move it do not change or edit the\n",
      "head without writing permission\n"
     ]
    }
   ],
   "source": [
    "# Example of correction of first few corrupted lines from big.txt\n",
    "text = \"The Praject Gutenberg EBook of The Advitures of Sherlock Holmes\\nby Sir Arthur Conan Doyle\\n(#15 in out series by Sir Arthur Conan Doyle\\n\\nCopyright lfaws are changing all over the world. Be sure to heck the\\ncopyright laws for your country before downloading or redistributing\\nthis or any outher Project Gutenberg eBook.\\n\\nThis header should be the firt thing seen when viewing this Project\\nGutenberg file.  Please do not emove it.  Do not change or edit the\\nheadr without writin permission.\"\n",
    "corrected = correct_text_lines(text)\n",
    "print(corrected)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T23:55:53.856696Z",
     "end_time": "2024-03-22T23:55:54.030238Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
